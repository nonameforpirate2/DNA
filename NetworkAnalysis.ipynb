{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Prediction from Scratch: the good, the bad and the ugly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is made to demonstrate the next skills:\n",
    "\n",
    "- Perform proper data preprocessing.\n",
    "- Perform natural language processing.\n",
    "- Use map and reduce operations.\n",
    "- Use streaming data.\n",
    "- Translate data into a graph and extract information from a network.\n",
    "- Apply machine learning\n",
    "- My ability to detect and handle imabalanced data.\n",
    "- Data Reduction using the PCA algorithm (The Principal Component Analysis)\n",
    "- Data Visualization with Matplotlib\n",
    "\n",
    "*From my point of view the application of graph theory is very handy to improve logistics in companies. Specially if we are talking about analyzing information from point A to point B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sc\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "#import networkx as nx\n",
    "import itertools\n",
    "import random\n",
    "import imblearn\n",
    "import category_encoders as ce\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from clustergrammer_widget import *\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Cancer Gene Detection\n",
    " \n",
    " To start coding. I am going to discuss a little bit about the idea behind the scence. I took this dataset available online which has the interactions within human proteins. I am interested in finding out those proteins which are more related to cancer given certain characteristics of the interaction network, to do this I need to detect whether the description containing the word cancer relates positively or negatively to the protein description, in order to classify them as cancer/no cancer(sentiment analysis). Then I am going to do natural language processing to add certain caracteristics given the protein description as an attribute to the node. Afterwards, I am going to proceed to perform some machine learning algorithm to find out those proteins interactions which tend to be, more exposed to turn into cancer given the characteristics of my network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "I took the dataset from the next link https://string-db.org/cgi/download?sessionId=%24input-%3E%7BsessionId%7D&species_text=Homo+sapiens, which takes into account only those proteins from Humans. I do have a list of links with the proteins and their interactions, together with their ID's and descriptions, I will try to find out the cancer labels by doing sentiment analysis over the descriptions. \n",
    "   a) First dataset contains the proteins with its descriptions.\n",
    "   b) Second dataset contains the protein interactions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Clean the file with the information of each protein.\n",
    "proteinsInfoOriginal = open(\"9606.protein.info.v11.0.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second dataset with protein interactions\n",
    "proteinNetwork = pd.read_csv('9606.protein.links.v11.0.txt', sep=' ', header='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NATURAL LANGUAGE PROCESSING: ASSINGING THE LABELS \n",
    "\n",
    "First we are going to build a dataset out of our txt file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataset into dataframe.\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def edit_line(line):\n",
    "    return str(line).split(\"\\t\")\n",
    "def tokenization(txt):\n",
    "    return word_tokenize(str(txt).lower())\n",
    "def lemmatization(txt):\n",
    "    return [WordNetLemmatizer().lemmatize(t) for t in txt]\n",
    "def stopwords_txt(txt):\n",
    "    return (set(txt)-stopwords)\n",
    "def contains_cancer(txt): \n",
    "    return 'cancer' in list(txt)\n",
    "\n",
    "data = list(map(edit_line,proteinsInfoOriginal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinDescription = pd.DataFrame(data[1:], columns = data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization process\n",
    "proteinDescription['annotation\\n'] = list(map(tokenization,proteinDescription['annotation\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LemmatizaproteinDescription['annotation\\n']tion process (I use lemmatization instead of steam, because is more precise)\n",
    "proteinDescription['annotation\\n'] = list(map(lemmatization,proteinDescription['annotation\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words erasing\n",
    "proteinDescription['annotation\\n'] = list(map(stopwords_txt,proteinDescription['annotation\\n']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to identify classify the proteins which contain the word cancer. \n",
    "# Those I am going to add a cancer label indicator on the description to filter and then perform sentiment analysis\n",
    "# over the descriptions which contain the word cancer. \n",
    "proteinDescription['cancer_label'] = list(map(contains_cancer,proteinDescription['annotation\\n']))\n",
    "#These are the proteins which are related to cancer by description, now I need to identify whether the content\n",
    "# is negative or positive.\n",
    "cancer_proteins = proteinDescription[proteinDescription['cancer_label'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-e2df2b7b983d>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cancer_proteins['index_list'] = cancer_proteins.index\n"
     ]
    }
   ],
   "source": [
    "cancer_proteins['index_list'] = cancer_proteins.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cancer = list(cancer_proteins['index_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteinDescription_x = pd.DataFrame(data[1:], columns = data[0])\n",
    "proteinDescription_x = proteinDescription_x[ proteinDescription_x.index.isin(list_cancer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I got the dataset with proteins related to cancer for manually map words/tokens related to cancer as positive or negative. \n",
    "proteinDescription_x.to_csv(\"cancer_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opinion Lexicon Generation: Dictionary based approach\n",
    "\n",
    "To create my opinion words. I inspect manually the 169 descriptions related to cancer and annotated the words related positive or negative to cancer. I do have my two lists and I am going to increase my set of opinion words using the dictionary based approach described in https://www.cs.uic.edu/~liub/FBS/NLP-handbook-sentiment-analysis.pdf. Due to my application, which is to define concrete descriptions, I decided to iterate one time. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proteins which protect from cancer\n",
    "pos_words = list(['protect', 'least', 'preventing', 'decreases', 'stabilizes', 'inhibits', 'suppress', 'mediated','suppressor', 'degrades', 'repressor', 'mediated', 'resistance'])\n",
    "#Proteins which produce cancer\n",
    "neg_words = list(['proliferation', 'promote', 'transition', 'differentation', 'malignancy','susceptibility','progression', 'metastasis', 'inactive', 'progenitor', 'disease', 'headcase', 'associated', 'downstream', 'repeat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSynonims(word):\n",
    "    syn = []\n",
    "    for word in wordnet.synsets(word):\n",
    "        for lemma in word.lemmas():\n",
    "            syn.append(lemma.name())\n",
    "    return syn\n",
    "#List of words gathered out of visual inspection. \n",
    "synonims_pos = []\n",
    "synonims_neg = []\n",
    "\n",
    "for word in pos_words:\n",
    "    synonims_pos.append(getSynonims(word))\n",
    "for word in neg_words:\n",
    "    synonims_neg.append(getSynonims(word))\n",
    "pos_words_list = set(list(itertools.chain(*synonims_pos)))\n",
    "neg_words_list = set(list(itertools.chain(*synonims_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some data cleaning on the lexicon lists. Because not all the synonim words retrieved are useful. \n",
    "pos_words_list = list(['arbitrate',\n",
    " 'brace',\n",
    " 'decrease',\n",
    " 'decrement',\n",
    " 'degrade',\n",
    " 'demean',\n",
    " 'diminish',\n",
    " 'diminution',\n",
    " 'drop-off',\n",
    " 'electric_resistance',\n",
    " 'electrical_resistance',\n",
    " 'fall',\n",
    " 'forbid',\n",
    " 'immunity',\n",
    " 'impedance',\n",
    " 'inhibit',\n",
    " 'intercede',\n",
    " 'intermediate',\n",
    " 'keep',\n",
    " 'least',\n",
    " 'lessen',\n",
    " 'lessening',\n",
    " 'liaise',\n",
    " 'mediate',\n",
    " 'mediated',\n",
    " 'minify',\n",
    " 'ohmic_resistance',\n",
    " 'opposition',\n",
    " 'oppress',\n",
    " 'preclude',\n",
    " 'prevent',\n",
    " 'protect',\n",
    " 'put_down',\n",
    " 'reduction',\n",
    " 'repress',\n",
    " 'represser',\n",
    " 'repressor',\n",
    " 'resistance',\n",
    " 'resistivity',\n",
    " 'resistor',\n",
    " 'stabilise',\n",
    " 'stabilize',\n",
    " 'stamp_down',\n",
    " 'steady',\n",
    " 'step-down',\n",
    " 'subdue',\n",
    " 'suppress',\n",
    " 'suppresser',\n",
    " 'suppresser_gene',\n",
    " 'suppressor',\n",
    " 'suppressor_gene',\n",
    " 'take_down',\n",
    " 'to_the_lowest_degree',\n",
    " 'underground'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words_list = list(['advance',\n",
    " 'advancement',\n",
    " 'advertise',\n",
    " 'advertize',\n",
    " 'affiliate',\n",
    " 'associate',\n",
    " 'assort',\n",
    " 'boost',\n",
    " 'changeover',\n",
    " 'colligate',\n",
    " 'connect',\n",
    " 'consort',\n",
    " 'conversion',\n",
    " 'disease',\n",
    " 'dormant',\n",
    " 'downriver',\n",
    " 'downstream',\n",
    " 'elevate',\n",
    " 'forward_motion',\n",
    " 'further',\n",
    " 'inactive',\n",
    " 'ingeminate',\n",
    " 'iterate',\n",
    " 'kick_upstairs',\n",
    " 'link',\n",
    " 'link_up',\n",
    " 'malignance',\n",
    " 'malignancy',\n",
    " 'malignity',\n",
    " 'metastasis',\n",
    " 'modulation',\n",
    " 'motionless',\n",
    " 'onward_motion',\n",
    " 'passive',\n",
    " 'patterned_advance',\n",
    " 'primogenitor',\n",
    " 'procession',\n",
    " 'progenitor',\n",
    " 'progress',\n",
    " 'progression',\n",
    " 'proliferation',\n",
    " 'promote',\n",
    " 'push',\n",
    " 'raise',\n",
    " 'recur',\n",
    " 'reduplicate',\n",
    " 'reiterate',\n",
    " 'relate',\n",
    " 'repeat',\n",
    " 'repetition',\n",
    " 'replicate',\n",
    " 'reprise',\n",
    " 'reprize',\n",
    " 'restate',\n",
    " 'retell',\n",
    " 'static',\n",
    " 'still',\n",
    " 'susceptibility',\n",
    " 'susceptibleness',\n",
    " 'take_over',\n",
    " 'tie_in',\n",
    " 'transition',\n",
    " 'upgrade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I am going to apply pos-tag to my words, because I need to keep adjectives, adverbs and nouns. I include nouns \n",
    "# because I am working with descriptions and nouns attach with negative cancer description are crucial. \n",
    "neg_tags = nltk.pos_tag(neg_words_list)\n",
    "pos_tags = nltk.pos_tag(pos_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tags = [item for item in neg_tags if item[1] != 'VB']\n",
    "pos_tags = [item for item in pos_tags if item[1] != 'VB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(tags_descriptions):\n",
    "    array_dummy = []\n",
    "    for element in tags_descriptions:\n",
    "        tags = nltk.pos_tag(list(element))\n",
    "        array_dummy.append(tags)\n",
    "    return array_dummy\n",
    "def pos_tags_filter(tags_array):\n",
    "    array_dummy = []\n",
    "    for element in tags_array:\n",
    "        array_dummy.append([item[0] for item in element if item[1] == 'JJ' or item[1] == 'NN' or item[1] == 'RB'])\n",
    "    return array_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding postag to my descriptions and filtering verbs.\n",
    "proteinDescription['annotation\\n'] = get_pos_tags(proteinDescription['annotation\\n'])\n",
    "#left the pos tag out of my array\n",
    "proteinDescription['annotation\\n'] = pos_tags_filter(proteinDescription['annotation\\n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_external_id</th>\n",
       "      <th>preferred_name</th>\n",
       "      <th>protein_size</th>\n",
       "      <th>annotation\\n</th>\n",
       "      <th>cancer_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>9606.ENSP00000064778</td>\n",
       "      <td>FAM168A</td>\n",
       "      <td>244</td>\n",
       "      <td>[pathway, cancer, degradation, fam168a, polb, ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>9606.ENSP00000167106</td>\n",
       "      <td>VASH1</td>\n",
       "      <td>365</td>\n",
       "      <td>[proliferation, cancer, effect, macrophage, sm...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>9606.ENSP00000201031</td>\n",
       "      <td>TFAP2C</td>\n",
       "      <td>450</td>\n",
       "      <td>[sequence, consensus, suppress, large, neural,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>9606.ENSP00000216807</td>\n",
       "      <td>BRMS1L</td>\n",
       "      <td>323</td>\n",
       "      <td>[histone, cancer, line, -dependent, activity, ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>9606.ENSP00000219022</td>\n",
       "      <td>OLFM4</td>\n",
       "      <td>510</td>\n",
       "      <td>[play, proliferation, g2/m, cancer, pancreatic...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18144</th>\n",
       "      <td>9606.ENSP00000463027</td>\n",
       "      <td>NCOA4</td>\n",
       "      <td>650</td>\n",
       "      <td>[cancer, peroxisome, coactivator, activity, pr...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18188</th>\n",
       "      <td>9606.ENSP00000464162</td>\n",
       "      <td>GREB1L</td>\n",
       "      <td>1923</td>\n",
       "      <td>[family, breast, growth, regulation, cancer, g...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18533</th>\n",
       "      <td>9606.ENSP00000473553</td>\n",
       "      <td>CSNK2A3</td>\n",
       "      <td>391</td>\n",
       "      <td>[play, residue, proliferation, suppressor, lar...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19289</th>\n",
       "      <td>9606.ENSP00000484135</td>\n",
       "      <td>COLCA2</td>\n",
       "      <td>251</td>\n",
       "      <td>[colorectal, cancer]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19308</th>\n",
       "      <td>9606.ENSP00000484443</td>\n",
       "      <td>NEK3</td>\n",
       "      <td>506</td>\n",
       "      <td>[activation, morphogenesis, polarity, kinase, ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        protein_external_id preferred_name protein_size  \\\n",
       "102    9606.ENSP00000064778        FAM168A          244   \n",
       "156    9606.ENSP00000167106          VASH1          365   \n",
       "241    9606.ENSP00000201031         TFAP2C          450   \n",
       "441    9606.ENSP00000216807         BRMS1L          323   \n",
       "514    9606.ENSP00000219022          OLFM4          510   \n",
       "...                     ...            ...          ...   \n",
       "18144  9606.ENSP00000463027          NCOA4          650   \n",
       "18188  9606.ENSP00000464162         GREB1L         1923   \n",
       "18533  9606.ENSP00000473553        CSNK2A3          391   \n",
       "19289  9606.ENSP00000484135         COLCA2          251   \n",
       "19308  9606.ENSP00000484443           NEK3          506   \n",
       "\n",
       "                                            annotation\\n  cancer_label  \n",
       "102    [pathway, cancer, degradation, fam168a, polb, ...          True  \n",
       "156    [proliferation, cancer, effect, macrophage, sm...          True  \n",
       "241    [sequence, consensus, suppress, large, neural,...          True  \n",
       "441    [histone, cancer, line, -dependent, activity, ...          True  \n",
       "514    [play, proliferation, g2/m, cancer, pancreatic...          True  \n",
       "...                                                  ...           ...  \n",
       "18144  [cancer, peroxisome, coactivator, activity, pr...          True  \n",
       "18188  [family, breast, growth, regulation, cancer, g...          True  \n",
       "18533  [play, residue, proliferation, suppressor, lar...          True  \n",
       "19289                               [colorectal, cancer]          True  \n",
       "19308  [activation, morphogenesis, polarity, kinase, ...          True  \n",
       "\n",
       "[169 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteinDescription[proteinDescription['cancer_label'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to assign to my lexicon a weight of importance to each of my lexicon words to define which is the most negative and the most positive word to do this, I am going to proceed to check the frequency of my words in my dataset. Thus, I am going to use map and reduce operations from pyspark to count the frequency of my lexicon words in my text and then be able to assign a preference over each most negative words and most positive words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By visual inspection and the frequency of my words, I will assign categories on my words from 1-5 (stars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(array):\n",
    "    words = \"\"\n",
    "    for item in array:\n",
    "        for word in item:\n",
    "            words += ' ' + word\n",
    "    return words\n",
    "        \n",
    "words = get_words(proteinDescription['annotation\\n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"words.txt\", \"w\")\n",
    "n = text_file.write(words)\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis over Descriptions: Lexicon Based Approach\n",
    "\n",
    "Given that I am analysing descriptions, which are all address to proteins related to cancer. I need to identify whether my word cancer relates positive on the text or negative. Thus, I will infer that I am dealing with a problem involved in mining direct opinions with explicit properties. In previous steps I already perform my tokenization lemmatization and stop words erasing. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197816"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-64-342bc57a750e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-64-342bc57a750e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    word count\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# to do add work count for map-reduce.\n",
    "word count\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "#file = sc.textFile(words)\n",
    "#rdd = sc.parallelize(\"words.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Working with the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the process, I am going to work with the data available, to build a network and extract properties out of it. There are three different ways to perform the task. In this assignment, I will address all of them to produce a final dataset, where I will have a protein with information about the amount of neighbors, how much information it sends to other proteins, how much information it receives from other proteins, special properties from the nodes (closeness, betweenness and centrality degree) and properties from the transmission action itself (neighborhood, neighborhood_transferred, fusion, cooccurence, homology, coexpression, coexpression_transferred, experiments, experiments_transferred, database, database_transferred, textmining, textmining_transferred, combined_score), which were obtained from the network description dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that I am going to do is to, define data frames which contain the nodes and the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(np.array(proteinCancerFinal['protein_external_id']))\n",
    "edges = list(tuple(np.array(pd.concat([proteinNetwork['protein1'], proteinNetwork['protein2']], axis=1, sort = False))))\n",
    "print(type(proteinCancerFinal['protein_external_id']))\n",
    "pd.DataFrame(proteinCancerFinal['protein_external_id']).to_csv('nodes.csv')\n",
    "pd.concat([proteinNetwork['protein1'], proteinNetwork['protein2']], axis=1, sort = False).to_csv(\"edges.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Graph (Part 1): NetworkX Library and GraphFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used first NetworkX library to extract properties from the network. I created a graph with the nodes and edges generated above, and I wanted to extract specific information from the network such as:\n",
    "- Number of neighbors per node.\n",
    "- Number of outDegrees per node (number of times that information is sent to another node).\n",
    "- Number of inDegrees (number of times that information is received from another node).\n",
    "- Centrality degree (location of the node within the network).\n",
    "- Special properties from a node for a given network ([betweenness](https://en.m.wikipedia.org/wiki/Betweenness_cetrality) and [closeness](https://neo4j.com/docs/graph-algorithms/current/algorithms/closeness-centrality/))\n",
    "\n",
    "From the previous information, I inferred that the nodes in the network were around 20,000 and the edges (amount of information transmitted) were 11,000,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My experience with NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used my laptop to extract information from the network at first instant. However, due to the graph extension and the software architecture of the NetworkX library, turned out to be too complex to extract the desire features (centrality degree, closeness and betweenness).\n",
    "\n",
    "In my studies at TU Delft I learned to use AWS. Thus, I decided to run the code in a AWS Jupyter Notebook with an EMR cluster. For the AWS configuration I used the S3 Bucket, an r3.xlarge instance (good price vs computational power) and a Jupyter Notebook (AWS). My findings were quite negative. Unfortunately, even in the cluster the properties were not extracted. I concluded that, even if the python library is called from a cluster, the computational cost of finding out the properties of each node, in a graph of around 20,000 nodes and 11 million edges turns out to be too complex. The involved library architecture does not support to work with too complex datasets, I will describe NetworkX as a good library for small datasets. The option of using GraphX from spark (well known for big data) does not support the graph property extraction such as closeness, betweenness etc. Thus, I decided to proceed with using pyspark and aggregating the edge data per node with map and reduce operations, using the network dataset (see [Working with the Graph (part2)]). From my personal point of view, finding the properties of the network and the data aggregation from the edge information will be perfect to strongly predict cancer. However, due to the current technology. I ended up using NetworkX only to extract the amount of neighbors per node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO IDENTIFY CLIQUES TO MAKE EFFICIENT THE COMPUTATION OF METRICS AND CHECK WHETHER A CLIQUE MIGHT BE A STRONG INDICATOR OF CANCER. GIVEN THAT I AM IN A NEIGHBORHOOD WITH CANCER, IT IS MORE PROBABLE THAT I DO HAVE CANCER. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell represents the code on the aws Jupyter Notebook previously configured to work with the EMR cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein1</th>\n",
       "      <th>protein2</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000272298</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000253401</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000401445</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000418915</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9606.ENSP00000000233</td>\n",
       "      <td>9606.ENSP00000327801</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11759449</th>\n",
       "      <td>9606.ENSP00000485678</td>\n",
       "      <td>9606.ENSP00000310488</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11759450</th>\n",
       "      <td>9606.ENSP00000485678</td>\n",
       "      <td>9606.ENSP00000342448</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11759451</th>\n",
       "      <td>9606.ENSP00000485678</td>\n",
       "      <td>9606.ENSP00000350222</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11759452</th>\n",
       "      <td>9606.ENSP00000485678</td>\n",
       "      <td>9606.ENSP00000367590</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11759453</th>\n",
       "      <td>9606.ENSP00000485678</td>\n",
       "      <td>9606.ENSP00000349930</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11759454 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      protein1              protein2  combined_score\n",
       "0         9606.ENSP00000000233  9606.ENSP00000272298             490\n",
       "1         9606.ENSP00000000233  9606.ENSP00000253401             198\n",
       "2         9606.ENSP00000000233  9606.ENSP00000401445             159\n",
       "3         9606.ENSP00000000233  9606.ENSP00000418915             606\n",
       "4         9606.ENSP00000000233  9606.ENSP00000327801             167\n",
       "...                        ...                   ...             ...\n",
       "11759449  9606.ENSP00000485678  9606.ENSP00000310488             167\n",
       "11759450  9606.ENSP00000485678  9606.ENSP00000342448             175\n",
       "11759451  9606.ENSP00000485678  9606.ENSP00000350222             195\n",
       "11759452  9606.ENSP00000485678  9606.ENSP00000367590             900\n",
       "11759453  9606.ENSP00000485678  9606.ENSP00000349930             213\n",
       "\n",
       "[11759454 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteinNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19354"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='networkanalysis'\n",
    "nodes_data = 'nodes.csv'\n",
    "edges_data = 'edges.csv'\n",
    "nodes_data = 's3://{}/{}'.format(bucket, nodes_data)\n",
    "edges_data = 's3://{}/{}'.format(bucket, edges_data)\n",
    "edges = list(tuple(np.array(pd.read_csv(edges_data).iloc[:,1:])))\n",
    "nodes = list(pd.read_csv(nodes_data).iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the graph with networks as nx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-417f9ba660d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Create graph object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#getting list of nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproteinNetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'protein1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproteinNetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'protein2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#getting list of edges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "#Create graph object. \n",
    "G = nx.DiGraph()\n",
    "#getting list of nodes\n",
    "nodes = list(set(proteinNetwork['protein1'].append(proteinNetwork['protein2'])))\n",
    "#getting list of edges\n",
    "edges = list(set(tuple(proteinNetwork['protein1'],proteinNetwork['protein2'])))\n",
    "G.add_nodes_from(nodes_data)\n",
    "G.add_edges_from(edges_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Nodes \" + str(G.number_of_nodes()))\n",
    "print(\"Number of Directed Edges: \" + str(G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = []\n",
    "centralityDegree = []\n",
    "closeness = []\n",
    "betweeness = []\n",
    "inDegree = []\n",
    "outDegree = []\n",
    "# Neighbors\n",
    "for node in nodes:\n",
    "    neighbors.append(len(list(G.successors(node))))\n",
    "    inDegree.append(G.in_degree(node))\n",
    "    outDegree.append(G.out_degree(node))\n",
    "    \n",
    "# Run the below commands in aws and my personal computer for several hours. It never finished.\n",
    "# Network size 20000 nodes and 11 million edges. \n",
    "    \n",
    "#closenessDict = nx.closeness_centrality(G)\n",
    "#betweennessDict = nx.betweenness_centrality(G)\n",
    "#centralityDegreeDict = nx.degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([proteinCancerFinal['protein_external_id'],pd.DataFrame(neighbors)], axis=1, sort=False).to_csv(\"neighbors.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the computational cost I only extract the amount of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Experience with GraphFrames (Pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still wanted to have more network properties to train the machine learning algorithm. Thus, I found out the [ GraphFrames](https://graphframes.github.io/graphframes/docs/_site/user-guide.html#triangle-count) library, which works well with SQL-Pyspark. I used the library to extract the outDegrees and inDegrees and the number of triangles passing to each node. I can describe the library to be quite handy to process bigger files, but still they do not offer the previous mentioned properties (Betweenness, Closenness and Centrality Degree). The reason for that is the complexity involved in computing them. Overall, I extracted a dataset where I had a protein id with outDegrees, inDegrees and the triangle count(pending). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import * #follow the documentation to open pyspark-shell with graphframes\n",
    "edges = sqlContext.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(\"/home/abi/Documents/Network Analysis/edges.csv\")\n",
    "nodes = sqlContext.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(\"/home/abi/Documents/Network Analysis/nodes.csv\")\n",
    "#data cleaning\n",
    "newedges = edges[[\"protein1\",\"protein2\"]]\n",
    "newnodes = nodes[[\"protein_external_id\"]]\n",
    "#change newnodes header to \"id\" matter of the library graphframe specification. Read documentation.\n",
    "oldcolumnnodes = newnodes.schema.names\n",
    "newcolumnnodes = ['id']\n",
    "newnodes2 = reduce(lambda newnodes, idx: newnodes.withColumnRenamed(oldcolumnnodes[idx], newcolumnnodes[idx]), xrange(len(oldcolumnnodes)), newnodes)\n",
    "#change edges header to src and dst.\n",
    "oldcolumnedges = newedges.schema.names\n",
    "newcolumnedges = ['src', 'dst']\n",
    "newedges2 = reduce(lambda newedges, idx: newedges.withColumnRenamed(oldcolumnedges[idx], newcolumnedges[idx]), xrange(len(oldcolumnedges)), newedges)\n",
    "g = GraphFrame(newnodes2,newedges2)\n",
    "vertexInDegrees = g.inDegrees\n",
    "vertexOutDegrees = g.outDegrees\n",
    "vertexOutDegrees.toPandas().to_csv(\"vertexOutDegrees.csv\")\n",
    "vertexInDegrees.toPandas().to_csv(\"vertexInDegrees.csv\")\n",
    "triangles = g.triangleCount()\n",
    "triangles.toPandas().to_csv(\"vertexTriangles.csv\")\n",
    "#join Data by key, which is equal to the protein tag.\n",
    "#A_B = A.join(B, A.id == B.id).select(A.*, B.b1, B.b2)\n",
    "joinedDegrees = vertexInDegrees.join(vertexOutDegrees,vertexOutDegrees.id == vertexInDegrees.id)\n",
    "#change schema due to id repetition.\n",
    "oldcolumndegrees = joinedDegrees.schema.names\n",
    "newcolumndegrees = ['id','inDegree','id1','outDegree']\n",
    "joinedDegrees2 = reduce(lambda joinedDegrees, idx: joinedDegrees.withColumnRenamed(oldcolumndegrees[idx], newcolumndegrees[idx]), xrange(len(oldcolumndegrees)), joinedDegrees)\n",
    "#drop id1\n",
    "pandasclean = joinedDegrees2.toPandas()\n",
    "pandas = pandasclean[['id','inDegree','outDegree']]\n",
    "pandas.to_csv('degrees.csv')\n",
    "#THE INDEGREES AND OUT DEGREES TURN OUT TO BE THE SAME. THIS FACT IS A PROOF OF HOW WONDERFUL\n",
    "#AND PERFECT THE ENGINEERING NATURE IS. HOWEVER, WHEN WORKING WITH OTHER TYPES OF NETWORKS THE OUTDEGREES AND\n",
    "#INDEGREES ARE RARELY THE SAME\n",
    "joinTrianglesVertexIn = triangles.join(vertexInDegrees, vertexInDegrees.id == triangles.id)\n",
    "joinTrianglesVertexIn.toPandas().to_csv(\"firstjoin.csv\")\n",
    "secondJoin = rddJoin.join(vertexOutDegrees, vertexOutDegrees.id == rddJoin.id)\n",
    "secondJoin.toPandas().to_csv(\"secondJoin.csv\")\n",
    "#*This join operations were done witth rdds, I am aware that it can be done as well by using join alike SQL operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Summary\n",
    "\n",
    "Thanks to NetworkX, I extracted the neighbors per node.\n",
    "Thanks to GraphFrames I got the inDegree, outDegree and triangle count for each node.\n",
    "The resulted format of the dataset is: \n",
    "\n",
    "node, number_of_neighbors, outDegree, inDegree, triangle_count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Graph (Part 2): Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network file was not really big, it had a size of only 1 gb. The problem is that it was too long. Taking into account the hardware side and how registers need a space in a RAM memory to perform operations 11000000 of registers are a problem and cannot be proccessed at once by any small device pay-able by a regular human (Of course until we manage to find out the secrets of the quantum computing, but I will leave that for the future :) ). Thus, I decided to use Spark Streaming. It basically consists in reading your data by chunks of information. In my case, nicely break my long data into small subsets that I can process with spark (each chunk at a time). Below, you can appreciate the code that I created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My intention with the code below was to produce an aggregation of the \"9606.protein.links.full.v11.0.txt\"(edge description data) in such way that I will end up producing a dataset including a protein name, plus the sum of each of the mentioned properties per node. In my case, I took into account the information transmitted from protein 1 to protein 2. Thus, I deleted protein 2 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add, sub\n",
    "from time import sleep\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from os import walk\n",
    "\n",
    "sc = SparkContext(appName=\"streamingProteins\")\n",
    "ssc = StreamingContext(sc,5)\n",
    "#Reshape Data\n",
    "file = open(\"/home/abi/Documents/Network Analysis/9606.protein.links.full.v11.0.txt\", \"r\").readlines()\n",
    "del file[0]\n",
    "def cleanStr(str):\n",
    "    newStr = str.replace(\"\\n\",\"\")\n",
    "    newStr2 = newStr.replace(newStr[21:42],\"\")\n",
    "    return newStr2\n",
    "newFile = map(cleanStr, file)\n",
    "#write Data to new txt file\n",
    "with open('proteinTransmission.txt', 'w') as filehandle:\n",
    "    for item in newFile:\n",
    "        filehandle.write('%s\\n' % item)\n",
    "\n",
    "#create definition to save outputdata\n",
    "def saveResults(rdd):\n",
    "    if not rdd.isEmpty():\n",
    "        rdd.toDF([\"protein1\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\"]).write.save(\"rddsprocess\", format=\"csv\", mode=\"append\")\n",
    "#Load proteinTransmission file\n",
    "proteinTransmission = open(\"proteinTransmission.csv\", \"r\").readlines()\n",
    "#create file to store the rdds manually file in the spark folder with name rddsprocess.txt\n",
    "#create rdd array like = queue \n",
    "rddQueue = []\n",
    "# store in queue the information batches\n",
    "for batch in proteinTransmission:\n",
    "    rddQueue += [ssc.sparkContext.parallelize(batch)]\n",
    "\n",
    "inputStream = ssc.queueStream(rddQueue)\n",
    "inputStreamSplit = inputStream.map(lambda x: x.split(\",\"))\n",
    "inputStreamTuple = inputStreamSplit.map(lambda x: (x[0],(x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10],x[11],x[12],x[13],x[14])))\n",
    "inputStreamInt = inputStreamTuple.map(lambda x: (x[0], [int(i) for i in x[1]]))\n",
    "totalStream = inputStreamInt.reduceByKey(lambda a,b: (a[0]+b[0],a[1]+b[1], a[2]+b[2],a[3]+b[3],a[4]+b[4],a[5]+b[5],a[6]+b[6],a[7]+b[7],a[8]+b[8],a[9]+b[9],a[10]+b[10],a[12]+b[12],a[13]+b[13],a[14]+b[14]))\n",
    "totalStream.foreachRDD(saveResults)\n",
    "\n",
    "scc.start()\n",
    "scc.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with the Graph (Part 3): The Rudymentary Way with Pyspark and Map and Reduce Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with small computer, I decided to break the file (using os), into several files and perform data aggregation over each file to produce a reduced file. Later, I used all the reduced file to produced a second reduction and finally get the aggregated data of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the amount of lines that my chunk files are going to have \n",
    "linesPerFile = 5000\n",
    "smallfile = None\n",
    "with open(\"proteinTransmission.txt\") as file:\n",
    "    for lineno, line in enumerate(file):\n",
    "        if lineno % linesPerFile == 0:\n",
    "            if smallfile:\n",
    "                smallfile.close()\n",
    "            smallName = \"filereduction/smallfile_{}.txt\".format(lineno + linesPerFile)\n",
    "            smallfile = open(smallName, \"w\")\n",
    "        smallfile.write(line)\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "def changestr(str):\n",
    "    newstr = str.replace(\"\\n\",\"\")\n",
    "    return str\n",
    "#iteration over the documents to perform map and reduce operation over each of them and print the reduced rdd to \n",
    "#a newreducefile.\n",
    "breakedData = os.listdir(\"/home/abi/spark/spark-2.4.3-bin-hadoop2.7/filereduction\")\n",
    "for file in breakedData:\n",
    "    workfile = \"filereduction/\" + file\n",
    "    workfilelist = open(workfile, \"r\").readlines()\n",
    "    newworkfilelist = map(changestr, workfilelist)\n",
    "    firstRdd = sc.parallelize(newworkfilelist)\n",
    "    tupleRDD = firstRdd.map(lambda x: x.split(\" \"))\n",
    "    tupleRdd = tupleRDD.map(lambda x: (x[0],(x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10],x[11],x[12],x[13],x[14])))\n",
    "    intrdd = tupleRdd.map(lambda x: (x[0], [int(i) for i in x[1]]))\n",
    "    #Personal comment: I still do not understand why it works with up to 13 instead of 14. But it works that way.\n",
    "    total = intrdd.reduceByKey(lambda a,b: (a[0]+b[0],a[1]+b[1],a[2]+b[2],a[3]+b[3],a[4]+b[4],a[5]+b[5],a[6]+b[6],a[7]+b[7],a[8]+b[8],a[9]+b[9],a[10]+b[10],b[11]+b[11],a[12]+b[12],a[13]+b[13]))\n",
    "    newName = \"filereduction/reduced/\" + \"red-\" + file.replace(\"txt\",\"csv\")\n",
    "    df = spark.createDataFrame(total)\n",
    "    df.toPandas().to_csv(newName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the previous cell represented the aggregate data of each batch of information. In order to fully aggregate the data, it is necessary to append all the information together and to perform a map and reduce operation again. Each file resulted in a ~20 row key value aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Data Processing to aggregate the chunks of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is to combine all the reduced files into one reduced file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the paths from reduced folder\n",
    "reducedData = os.listdir(\"/home/abi/spark/spark-2.4.3-bin-hadoop2.7/filereduction/reduced\")\n",
    "#create manually a file called result.txt contained in the reduced folder\n",
    "#save full paths of reduced documents to a list\n",
    "def updateListPath(pathDoc):\n",
    "    str = \"/home/abi/spark/spark-2.4.3-bin-hadoop2.7/filereduction/reduced/\" + pathDoc\n",
    "    return str\n",
    "#Update path list with the full path, initially it only contains the file names.\n",
    "updated = map(updateListPath,reducedData)\n",
    "\n",
    "#Append all the txt files together into one txt file.\n",
    "with open((\"/home/abi/spark/spark-2.4.3-bin-hadoop2.7/filereduction/reduced/reduced.txt\"),\"w\") as result:\n",
    "    for file in updated:\n",
    "        for line in open(file, \"r\"):\n",
    "            result.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code is to reduce all the information in the file. = More map and reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing again. Now I had to clean the data again because the spark data frame returned the file with another format.\n",
    "#load the resulted file and delete rows with unnecessary strings.\n",
    "reducedDataDf = spark.read.csv(\"/home/abi/spark/spark-2.4.3-bin-hadoop2.7/filereduction/reduced/reduced.txt\")\n",
    "#delete rows with null values and delete column _c0 index from the previous datasets.\n",
    "reducedDfDeleteRow = reducedDataDf.filter(reducedDataDf._c0 != 'null').drop(reducedDataDf._c0)\n",
    "#print the table as csv file to perform text processing over the values.\n",
    "reducedPandas = reducedDfDeleteRow.toPandas()\n",
    "noisyTuples = list(reducedPandas[\"_c2\"])\n",
    "#clean column\n",
    "def cleanTup(str):\n",
    "    del1 = str.replace(\"Row(\",\"\")\n",
    "    del1 = del1.replace(\"_1=\",\"\")\n",
    "    del2 = del1.replace(\"_2=\",\"\")\n",
    "    del3 = del2.replace(\"_3=\",\"\")\n",
    "    del4 = del3.replace(\"_4=\",\"\")\n",
    "    del5 = del4.replace(\"_5=\",\"\")\n",
    "    del6 = del5.replace(\"_6=\",\"\")\n",
    "    del7 = del6.replace(\"_7=\",\"\")\n",
    "    del8 = del7.replace(\"_8=\",\"\")\n",
    "    del9 = del8.replace(\"_9=\",\"\")\n",
    "    del10 = del9.replace(\"_10=\",\"\")\n",
    "    del11 = del10.replace(\"_11=\",\"\")\n",
    "    del12 = del11.replace(\"_12=\",\"\")\n",
    "    del13 = del12.replace(\"_13=\",\"\")\n",
    "    del14 = del13.replace(\"_14=\",\"\")\n",
    "    del15 = del14.replace(\")\",\"\")\n",
    "    del15 = del15.replace(\"u'\",\"\")\n",
    "    return del15\n",
    "\n",
    "clean = map(cleanTup,noisyTuples)\n",
    "protein = list(reducedPandas['_c1'])\n",
    "df = pandas.DataFrame(clean,protein)\n",
    "#print csv ready to perform last step of map and reduce.\n",
    "df.to_csv(\"last_step_mp.csv\", sep=\" \")\n",
    "#load csv to perform last map and reduce operation.\n",
    "#clean data again.\n",
    "file = open(\"last_step_mp.csv\", \"r\").readlines()\n",
    "\n",
    "def cleanstr(str):\n",
    "    str = str.replace(\",\",\"\")\n",
    "    str2 = str.replace('\"',\"\")\n",
    "    str3 = str2.replace(\"\\n\",\"\")\n",
    "    return str3\n",
    "\n",
    "newFile = map(cleanstr,file)\n",
    "del newFile[0]\n",
    "\n",
    "#work with the map and reduce logic.\n",
    "firstRdd = sc.parallelize(newFile)\n",
    "tupleRDD = firstRdd.map(lambda x: x.split(\" \"))\n",
    "tupleRdd = tupleRDD.map(lambda x: (x[0],(x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10],x[11],x[12],x[13],x[14])))\n",
    "intrdd = tupleRdd.map(lambda x: (x[0], [int(i) for i in x[1]]))\n",
    "#Personal comment: I still do not understand why it works with up to 13 instead of 14. But it works that way.\n",
    "total = intrdd.reduceByKey(lambda a,b: (a[0]+b[0],a[1]+b[1],a[2]+b[2],a[3]+b[3],a[4]+b[4],a[5]+b[5],a[6]+b[6],a[7]+b[7],a[8]+b[8],a[9]+b[9],a[10]+b[10],b[11]+b[11],a[12]+b[12],a[13]+b[13]))\n",
    "df = spark.createDataFrame(total).toPandas().to_csv(\"nodesnetaggregation.csv\")\n",
    "\n",
    "#*Data cleaning might be less complicated by using sql alike tables or Tableau itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the map and reduce operations over the values, it was noted that only around 7000 nodes were involved in the network dataset. The result makes sense since all the possible proteins registered with the description are present in the first dataset. Now, I will aggregate the data to link the corresponding network properties with each protein. Overall, the four datasets that I am going to combine are the \"neighbors.csv\", \"degrees.csv\",\"finalData.csv\"(cancer label) and the \"nodesnetaggregation.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating the datasets for the machine learning algorithm (more map and reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I cleaned manually the files. Erasing unnecessary columns and renaming columns.\n",
    "# Creating a string list of the files\n",
    "degrees = open(\"degrees.csv\", \"r\").readlines()\n",
    "neighbors = open(\"neighbors.csv\",\"r\").readlines()\n",
    "nodesnet = open(\"nodesnetaggregation.csv\").readlines()\n",
    "cancer = open(\"finalData.csv\").readlines()\n",
    "\n",
    "# Erasing \"\\n\" from the list of strings\n",
    "def clean(str):\n",
    "    newstr2 = str.replace(\"\\n\",\"\")\n",
    "    return newstr2\n",
    "\n",
    "degreesClean = map(clean,degrees)\n",
    "neighborsClean = map(clean,neighbors)\n",
    "nodesnetClean = map(clean,nodesnet)\n",
    "cancerClean = map (clean,cancer)\n",
    "del degreesClean[0]\n",
    "del neighborsClean[0]\n",
    "del nodesnetClean[0]\n",
    "del cancerClean[0]\n",
    "# Creating first rdd of strings\n",
    "degreesRdd = sc.parallelize(degreesClean)\n",
    "neighborsRdd = sc.parallelize(neighborsClean)\n",
    "nodesnetRdd = sc.parallelize(nodesnetClean)\n",
    "cancerRdd = sc.parallelize(cancerClean)\n",
    "\n",
    "# Spliting first rdd by \",\"\n",
    "degreesRddSch =  degreesRdd.map(lambda x: x.split(\",\"))\n",
    "neighborsRddSch = neighborsRdd.map(lambda x: x.split(\",\"))\n",
    "nodesnetRddSch = nodesnetRdd.map(lambda x: x.split(\",\"))\n",
    "cancerRddSch = cancerRdd.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Turn tuple RDD to data frame.\n",
    "degreesTuple = degreesRddSch.map(lambda x: (x[0],(x[1],x[2])))\n",
    "neighborsTuple = neighborsRddSch.map(lambda x: (x[0],x[1]))\n",
    "nodesnetTuple = nodesnetRddSch.map(lambda x: (x[0],(x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10],x[11],x[12],x[13],x[14])))\n",
    "cancerTuple = cancerRddSch.map(lambda x: (x[0],x[1]))\n",
    "# Performing join operations over the RDDs.\n",
    "nodesdeg = nodesnetTuple.join(degreesTuple)\n",
    "#Transform to data frames the rdds.\n",
    "nodesnetDf = nodesnetTuple.toDF()\n",
    "degreesDf = degreesTuple.toDF()\n",
    "neighborsDf = neighborsTuple.toDF()\n",
    "cancerDf = cancerRddSch.toDF()\n",
    "#write union read file back clean data and create tuple rdd\n",
    "nodesDeg = nodesnetDf.join(degreesDf, nodesnetDf._1 == degreesDf._1).select(nodesnetDf[\"*\"],degreesDf[\"_2\"])\n",
    "nodesDegNeig = nodesDeg.join(neighborsDf, nodesDeg._1 == neighborsDf._1).select(nodesDeg[\"*\"], neighborsDf[\"_2\"])\n",
    "nodesDegNeigCan = nodesDegNeig.join(cancerDf, nodesDegNeig._1 == cancerDf._1).select(nodesDegNeig[\"*\"], cancerDf[\"_2\"])\n",
    "#Now we had a dataset with the form of protein Id, netproperties, indegrees, out degrees, amount of neighbors and cancer\n",
    "#I will print it and clean it again.\n",
    "#9606.ENSP00000257013\tRow(_1=u'0', _2=u'0', _3=u'0', _4=u'0', _5=u'3628', _6=u'7274', _7=u'239', _8=u'1874', _9=u'0', _10=u'0', _11=u'0', _12=u'1132', _13=u'7225', _14=u'34034')\tRow(_1=u'135', _2=u'135')\t135\t0\n",
    "nodesDegNeigCan.toPandas().to_csv(\"mlpreviousfinal.csv\")\n",
    "#manually delete first row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to the soup: Data Cleaning to get our final dataset to perform the SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, what I am going to do is preprocessing the output file again from the previous step to give it a shape of a proper matrix, for applying the machine learning algorithm. Afterwards, I am going to normalize the data and perform a PCA analysis, to find out the components/features that are valuable enough to be used in the machine learning algorithm. The principal component analysis algorithm is well known to be used as feature reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset as a list of strings\n",
    "datasemiFinal = open(\"/home/abi/spark/spark-2.4.3-bin-hadoop2.7/mlpreviousfinal.csv\",\"r\").readlines()\n",
    "#Clean the ugly string again T_T\n",
    "'9606.ENSP00000257013,\"Row(_1=u\\'0\\', _2=u\\'0\\', _3=u\\'0\\', _4=u\\'0\\', _5=u\\'3628\\', _6=u\\'7274\\', _7=u\\'239\\', _8=u\\'1874\\', _9=u\\'0\\', _10=u\\'0\\', _11=u\\'0\\', _12=u\\'1132\\', _13=u\\'7225\\', _14=u\\'34034\\')\",\"Row(_1=u\\'135\\', _2=u\\'135\\')\",135,0\\n'\n",
    "def cleanStr(str):\n",
    "    newstr = str.replace('\"',\"\")\n",
    "    str1 = newstr.replace('\\'',\"\")\n",
    "    str2 = str1.replace(\"Row(\",\"\")\n",
    "    str3 = str2.replace(\"_2=u\",\"\")\n",
    "    str4 = str3.replace(\"_3=u\",\"\")\n",
    "    str5 = str4.replace(\"_4=u\",\"\")\n",
    "    str6 = str5.replace(\"_5=u\",\"\")\n",
    "    str7 = str6.replace(\"_6=u\",\"\")\n",
    "    str8 = str7.replace(\"_7=u\",\"\")\n",
    "    str9 = str8.replace(\"_8=u\",\"\")\n",
    "    str10 = str9.replace(\"_9=u\",\"\")\n",
    "    str11 = str10.replace(\"_10=u\",\"\")\n",
    "    str12 = str11.replace(\"_11=u\",\"\")\n",
    "    str13 = str12.replace(\"_12=u\",\"\")\n",
    "    str14 = str13.replace(\"_13=u\",\"\")\n",
    "    str15 = str14.replace(\"_14=u\",\"\")\n",
    "    str16 = str15.replace(\"Row(_1=u\",\"\")\n",
    "    str17 = str16.replace(\"_2=u\",\"\")\n",
    "    str18 = str17.replace('\\')\"',\"\")\n",
    "    str19 = str18.replace(\"\\n\",\"\")\n",
    "    str20 = str19.replace(\")\",\"\")\n",
    "    str21 = str20.replace(\"_1=u\",\"\")\n",
    "    return str21\n",
    "datasemiFinalClean = map(cleanStr,datasemiFinal)\n",
    "#Turn into rdd to gave it a shape\n",
    "datasemiRdd = sc.parallelize(datasemiFinalClean).map(lambda x: x.split(\",\"))\n",
    "#Turn into pandas csv and print.\n",
    "datasemiRdd.toDF().toPandas().to_csv(\"machineLearningReady.csv\")\n",
    "#manually configure column names according to previous steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing manually columns the machineLearningReady.csv. Has the columns in the format below:\n",
    "- id\n",
    "- neighborhood\n",
    "- neighborhoodTransferred\n",
    "- fusion \n",
    "- coocurence\n",
    "- homology\n",
    "- coexpression\n",
    "- coexpressionTransferred\n",
    "- experiments\n",
    "- experimentsTransferred\n",
    "- database\n",
    "- databaseTransferred\n",
    "- combinedScores\n",
    "- neighbors\n",
    "- inDegree\n",
    "- outDegree\n",
    "- cancer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Data Preprocessing: Applying Normalization and Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I gave it a good shape to the data, I will proceed to prepare the clean data for the machine learning algorithm. Thus, I will apply normalization over all numerical features and encode labels of the proteins. Afterwards, I will apply a PCA algorithm over the dataset to verify whether it is necessary to use all the feratures or just a couple of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing the label encoding and normalization of the numerical values. Eliminate manually zero column.\n",
    "dataMl = pd.read_csv('machineLearningReady.csv')\n",
    "x = dataMl.iloc[:,1:].values\n",
    "x = StandardScaler().fit_transform(x)\n",
    "minmaxscaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = minmaxscaler.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration: Variable Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I would like to demonstrate the visualization of the feature values against the proteins with a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfVisualization = pd.DataFrame(dataMl.iloc[:,0])\n",
    "dfNormalization = pd.DataFrame(StandardScaler().fit_transform(dataMl.iloc[:,1:].values), columns = ['neighborhood','neighborhoodTransferred','fusion','coocurence','homology','coexpression','coexpressionTransferred','experiments','experimentsTransferred','delete','database','databaseTransferred','combinedScores','neighbors','inDegree','outDegree'\n",
    "])\n",
    "dfVisual = pd.concat([dfVisualization,dfNormalization], axis=1, sort=False).dropna()\n",
    "del dfVisual[\"delete\"]\n",
    "#net = Network(clustergrammer_widget)\n",
    "vis = dfVisual.iloc[:,1:].values\n",
    "cols = np.array(dfVisual['id'])\n",
    "ind = ['neighborhood','neighborhoodTransferred','fusion','coocurence','homology','coexpression','coexpressionTransferred','experiments','experimentsTransferred','database','databaseTransferred','combinedScores','neighbors','inDegree','outDegree'\n",
    "]\n",
    "vs = pd.DataFrame(vis.T, index = ind, columns = cols)\n",
    "vs.T.to_csv('vs_explore.csv')\n",
    "plt.pcolor(vs)\n",
    "plt.yticks(np.arange(0.5, len(vs.index), 1), vs.index)\n",
    "plt.xticks(np.arange(0.5, len(vs.columns), 1), vs.columns, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above displays the 7000 proteins with its corresponding values. The reader cannot really infer from the plot, how the cancer looks. Thus, below I sliced information in chunks of 25 proteins to be able to compare cancer against regular proteins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVisualization_z = pd.DataFrame(np.array(dataMl.iloc[475:500,0]))\n",
    "dfNormalization_z = pd.DataFrame(StandardScaler().fit_transform(dataMl.iloc[475:500,1:].values), columns = ['neighborhood','neighborhoodTransferred','fusion','coocurence','homology','coexpression','coexpressionTransferred','experiments','experimentsTransferred','delete','database','databaseTransferred','combinedScores','neighbors','inDegree','outDegree'\n",
    "])\n",
    "dfVisual_z = pd.concat([dfVisualization_z,dfNormalization_z], axis=1, sort=False).dropna()\n",
    "del dfVisual_z[\"delete\"]\n",
    "#net = Network(clustergrammer_widget)\n",
    "vis_z = dfVisual_z.iloc[:,1:].values\n",
    "cols = np.array(dfVisual_z[0])\n",
    "ind = ['neighborhood','neighborhoodTransferred','fusion','coocurence','homology','coexpression','coexpressionTransferred','experiments','experimentsTransferred','database','databaseTransferred','combinedScores','neighbors','inDegree','outDegree'\n",
    "]\n",
    "vs_z = pd.DataFrame(vis_z.T, index = ind, columns = cols)\n",
    "plt.pcolor(vs_z)\n",
    "plt.yticks(np.arange(0.5, len(vs_z.index), 1), vs_z.index)\n",
    "plt.xticks(np.arange(0.5, len(vs_z.columns), 1), vs_z.columns, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first chunk of 25 proteins shows clearly the influence of the outDegree feature to detect cancer. In this case protein \"9606.ENSP00000276480\" was detected as cancer. However, one slice of information does not confirm the visual exploration. Due to this reason, I plotted another two chunks for verification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfVisualization_m = pd.DataFrame(np.array(dataMl.iloc[575:600,0]))\n",
    "dfNormalization_m = pd.DataFrame(StandardScaler().fit_transform(dataMl.iloc[575:600,1:].values), columns = ['neighborhood','neighborhoodTransferred','fusion','coocurence','homology','coexpression','coexpressionTransferred','experiments','experimentsTransferred','delete','database','databaseTransferred','combinedScores','neighbors','inDegree','outDegree'\n",
    "])\n",
    "dfVisual_m = pd.concat([dfVisualization_m,dfNormalization_m], axis=1, sort=False).dropna()\n",
    "del dfVisual_m[\"delete\"]\n",
    "#net = Network(clustergrammer_widget)\n",
    "vis_m = dfVisual_m.iloc[:,1:].values\n",
    "cols = np.array(dfVisual_m[0])\n",
    "ind = ['neighborhood','neighborhoodTransferred','fusion','coocurence','homology','coexpression','coexpressionTransferred','experiments','experimentsTransferred','database','databaseTransferred','combinedScores','neighbors','inDegree','outDegree'\n",
    "]\n",
    "vs_m = pd.DataFrame(vis_m.T, index = ind, columns = cols)\n",
    "plt.pcolor(vs_m)\n",
    "plt.yticks(np.arange(0.5, len(vs_m.index), 1), vs_m.index)\n",
    "plt.xticks(np.arange(0.5, len(vs_m.columns), 1), vs_m.columns, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second chunk of 25 proteins confirms the influence of the outDegree feature to detect cancer. In this case protein \"9606.ENSP00000263083\" was detected as cancer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfVisualization_m = pd.DataFrame(np.array(dataMl.iloc[751:775,0]))\n",
    "dfNormalization_m = pd.DataFrame(StandardScaler().fit_transform(dataMl.iloc[751:775,1:].values), columns = ['neighborhood','neighborhoodTransferred','fusion','coocurence','homology','coexpression','coexpressionTransferred','experiments','experimentsTransferred','delete','database','databaseTransferred','combinedScores','neighbors','inDegree','outDegree'\n",
    "])\n",
    "dfVisual_m = pd.concat([dfVisualization_m,dfNormalization_m], axis=1, sort=False).dropna()\n",
    "del dfVisual_m[\"delete\"]\n",
    "#net = Network(clustergrammer_widget)\n",
    "vis_m = dfVisual_m.iloc[:,1:].values\n",
    "cols = np.array(dfVisual_m[0])\n",
    "ind = ['neighborhood','neighborhoodTransferred','fusion','coocurence','homology','coexpression','coexpressionTransferred','experiments','experimentsTransferred','database','databaseTransferred','combinedScores','neighbors','inDegree','outDegree'\n",
    "]\n",
    "vs_m = pd.DataFrame(vis_m.T, index = ind, columns = cols)\n",
    "plt.pcolor(vs_m)\n",
    "plt.yticks(np.arange(0.5, len(vs_m.index), 1), vs_m.index)\n",
    "plt.xticks(np.arange(0.5, len(vs_m.columns), 1), vs_m.columns, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third chunk of 25 proteins again confirms the influence of the outDegree feature to detect cancer. In this case protein \"9606.ENSP00000291906\" was detected as cancer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall from the visualization exploration, it is possible to infer the importance of analyzing from the network approach interaction between proteins. In this case I confirm the importance of the outDegree feature to train the machine learning algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network object\n",
    "net = Network(clustergrammer_widget)\n",
    "# load dataframe\n",
    "net.load_df(vs_m)\n",
    "# cluster using default parameters\n",
    "net.cluster(enrichrgram=True)\n",
    "# make the visualization\n",
    "net.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please check the Tableau Report for a complete detail of the data visualization exploration* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Component Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I am going to verify how many of the features affecting proteins are useful to detect cancer. Thus, the next step is to apply the principal component analysis to create feature reduction. In order to proceed with the PCA, I will need to know which independent variables are more significant in terms of variance. Thanks to the function \"explained_variance_ratio\", I will check the percentage of variance is represented by each variable. From my personal point of view, I like to reduce the variables up to the point where I am using around 70% of the system variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataMlPreprocess = pd.concat([xScaled,pd.DataFrame(dataMl.iloc[:,16])], axis = 1, sort = False)\n",
    "x_reduced = pd.read_csv('x_scaled_reduced.csv')\n",
    "y_org = pd.DataFrame(dataMl.iloc[:,16])\n",
    "#concat I need to erase rows with null values.\n",
    "cleanReduced = pd.concat([x_reduced,y_org], axis = 1, sort = False).dropna()\n",
    "x_reduced = cleanReduced.iloc[:,1:-1]\n",
    "y_reduced = cleanReduced.iloc[:,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amount of components decision: run pca with all the elements in the x_reduced and analyze the percentage of \n",
    "# variance over the dataset with the percentage of variance.\n",
    "pca = PCA(n_components = 12)\n",
    "principal = pca.fit_transform(np.array(x_reduced))\n",
    "x_org_pca = principal \n",
    "percentageVarianceVector = pca.explained_variance_ratio_\n",
    "percentageVarianceVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the percentage variance vector we can infer that working with 4 components will meet the requirement close to 70%. Thus, I will run the dimensionality reduction with 4 components. After deciding how many components to use, I will proceed with performing the imbalance data check to continue with the pca algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it to check whether the matrix was working properly.\n",
    "#np.linalg.svd(np.array(x_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Imbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure that the predictions are made properly, it is needed to know how our data looks. For example, if we have too few cancer properties, we will need to create a homogeneous dataset in order to test the algorithm. Otherwise, it might be biased due to the skewed data. The same case occurs in the anomaly detection for credit cards fraud, cases where the anomalies turn out to be around 5% or 2% of the total amount of data. \n",
    "Therefore, from the Machine Learning Dataset we want to identify how many positive cancer proteins do we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredCancer = pd.DataFrame(y_reduced).query(\"cancer == 1\") \n",
    "percentageCancer = (filteredCancer.count()/y_reduced.count())*100\n",
    "percentageCancer\n",
    "# Cancer = 0.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering the cancer proteins, I found out that only 135 are cancer out of 19518. That makes our data highly imbalanced, only 0.70% of the proteins represent cancer. There will be a need to use SMOTE (Synthetic Minority Over-sampling Technique) to give homogeneity to our dataset. According to [Imbalanced Learning](https://imbalanced-learn.org/en/stable/auto_examples/combine/plot_comparison_combine.html#sphx-glr-auto-examples-combine-plot-comparison-combine-py) we will need to apply the oversampling method to increase our skewed cancer data and an undersampling method to clean the noise generated on data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform resample minority\n",
    "#Increase the skewed cancer data to 20%.\n",
    "smote = SMOTE(sampling_strategy = 0.2)\n",
    "x_res, y_res = smote.fit_resample(x_reduced,y_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform undersample of majority (its a problem to compute such amount of data for the ML algorithm)\n",
    "under_sampling = RandomUnderSampler(sampling_strategy = 0.4)\n",
    "x_un, y_un = under_sampling.fit_resample(x_res,y_res)\n",
    "x_un[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Data Preprocessing: Rebuilding Dataset After Balancing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resampling, I will check again how the proportions are, and based on the results, change the sample ratios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rebuild = pd.DataFrame(x_un, columns = [\"0\",\"1\", \"2\", \"3\", \"4\", \"5\", \"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\"])\n",
    "y_rebuild = pd.DataFrame(y_un, columns = [\"cancer\"])\n",
    "rebuildCleanData = pd.concat([x_rebuild, y_rebuild], axis = 1, sort = False)\n",
    "rebuildCleanData.to_csv(\"rebuildmachinelearningready.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_cancer = len(rebuildCleanData.query(\"cancer == 1\"))\n",
    "total_data = len(rebuildCleanData)\n",
    "percentage = (float(amount_cancer)/float(total_data))*100\n",
    "# cancer = ~28%\n",
    "percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rebuilding the dataset, now we do have a balance cancer / protein information (cancer ~ 30%). Thus, I am able to proceed with the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithm (Supervised Learning) : SVM for the binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected the support vector machine algorithm (SVM) to perform the binary classification over the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate clean dataset into training set and test set.\n",
    "x_split = rebuildCleanData.iloc[:,:-1]\n",
    "y_split = rebuildCleanData.iloc[:,13]\n",
    "X_train, x_test, Y_train, y_test = train_test_split(x_split,y_split,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying resampling and spliting the dataset into training set and test set, I will apply the principal component analysis to reduce the independent variables to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 4)\n",
    "x_train_pca = pca.fit_transform(X_train)\n",
    "x_test_pca = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying machine learning algorithm\n",
    "clf = svm.SVC(kernel = 'linear', C=0.01)\n",
    "y_pred = clf.fit(x_train_pca, Y_train).predict(x_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation: F - Measure to check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do add K-FOLD CROSS VALIDATION\n",
    "conf = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Precision and Recall of the algorithm\n",
    "precision = float(conf[0,0])/float(conf[0,0] + conf[0,1])\n",
    "recall = float(conf[0,0])/float(conf[1,0] + conf[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Compute F-Measure\n",
    "f_measure = 2*((precision*recall)/(precision + recall)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_measure \n",
    "# ~83% The project has pending a cross Validation and a Grid Search to improve performance on the algorithm. (Waiting to work in a laptop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
